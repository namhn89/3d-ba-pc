{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  1590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f74b408c570>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import logging\n",
    "from visualization.visualization_utils import pyplot_draw_point_cloud\n",
    "from visualization.visualize_pointnet import make_one_critical\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dataset.mydataset import PoisonDataset\n",
    "from models.pointnet_cls import get_loss, get_model\n",
    "from config import *\n",
    "from visualization.customized_open3d import *\n",
    "from load_data import load_data\n",
    "import matplotlib.pyplot as plt\n",
    "import data_utils\n",
    "\n",
    "manualSeed = random.randint(1, 10000)  # fix seed\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    0: 'airplane',\n",
    "    1: 'bathtub',\n",
    "    2: 'bed',\n",
    "    3: 'bench',\n",
    "    4: 'bookshelf',\n",
    "    5: 'bottle',\n",
    "    6: 'bowl',\n",
    "    7: 'car',\n",
    "    8: 'chair',\n",
    "    9: 'cone',\n",
    "    10: 'cup',\n",
    "    11: 'curtain',\n",
    "    12: 'desk',\n",
    "    13: 'door',\n",
    "    14: 'dresser',\n",
    "    15: 'flower_pot',\n",
    "    16: 'glass_box',\n",
    "    17: 'guitar',\n",
    "    18: 'keyboard',\n",
    "    19: 'lamp',\n",
    "    20: 'laptop',\n",
    "    21: 'mantel',\n",
    "    22: 'monitor',\n",
    "    23: 'night_stand',\n",
    "    24: 'person',\n",
    "    25: 'piano',\n",
    "    26: 'plant',\n",
    "    27: 'radio',\n",
    "    28: 'range_hood',\n",
    "    29: 'sink',\n",
    "    30: 'sofa',\n",
    "    31: 'stairs',\n",
    "    32: 'stool',\n",
    "    33: 'table',\n",
    "    34: 'tent',\n",
    "    35: 'toilet',\n",
    "    36: 'tv_stand',\n",
    "    37: 'vase',\n",
    "    38: 'wardrobe',\n",
    "    39: 'xbox'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "log_dir = 'train_500_24_modelnet40'\n",
    "dataset = 'modelnet40'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "global x_train, y_train, x_test, y_test, num_classes\n",
    "if dataset == \"modelnet40\":\n",
    "    x_train, y_train, x_test, y_test = load_data()\n",
    "    num_classes = 40\n",
    "elif dataset == \"scanobjectnn_pb_t50_rs\":\n",
    "    x_train, y_train = data_utils.load_h5(\"data/h5_files/main_split/training_objectdataset_augmentedrot_scale75.h5\")\n",
    "    x_test, y_test = data_utils.load_h5(\"data/h5_files/main_split/test_objectdataset_augmentedrot_scale75.h5\")\n",
    "    y_train = np.reshape(y_train, newshape=(y_train.shape[0], 1))\n",
    "    y_test = np.reshape(y_test, newshape=(y_test.shape[0], 1))\n",
    "    num_classes = 15\n",
    "elif dataset == \"scanobjectnn_obj_bg\":\n",
    "    x_train, y_train = data_utils.load_h5(\"data/h5_files/main_split/training_objectdataset.h5\")\n",
    "    x_test, y_test = data_utils.load_h5(\"data/h5_files/main_split/test_objectdataset.h5\")\n",
    "    y_train = np.reshape(y_train, newshape=(y_train.shape[0], 1))\n",
    "    y_test = np.reshape(y_test, newshape=(y_test.shape[0], 1))\n",
    "    num_classes = 15\n",
    "elif dataset == \"scanobjectnn_pb_t50_r\":\n",
    "    x_train, y_train = data_utils.load_h5(\"data/h5_files/main_split/training_objectdataset_augmentedrot.h5\")\n",
    "    x_test, y_test = data_utils.load_h5(\"data/h5_files/main_split/test_objectdataset_augmentedrot.h5\")\n",
    "    y_train = np.reshape(y_train, newshape=(y_train.shape[0], 1))\n",
    "    y_test = np.reshape(y_test, newshape=(y_test.shape[0], 1))\n",
    "    num_classes = 15\n",
    "elif dataset == \"scanobjectnn_pb_t25_r\":\n",
    "    x_train, y_train = data_utils.load_h5(\"data/h5_files/main_split/training_objectdataset_augmented25rot.h5\")\n",
    "    x_test, y_test = data_utils.load_h5(\"data/h5_files/main_split/test_objectdataset_augmented25rot.h5\")\n",
    "    y_train = np.reshape(y_train, newshape=(y_train.shape[0], 1))\n",
    "    y_test = np.reshape(y_test, newshape=(y_test.shape[0], 1))\n",
    "    num_classes = 15\n",
    "elif dataset == \"scanobjectnn_pb_t25\":\n",
    "    x_train, y_train = data_utils.load_h5(\"data/h5_files/main_split/training_objectdataset_augmented25_norot.h5\")\n",
    "    x_test, y_test = data_utils.load_h5(\"data/h5_files/main_split/test_objectdataset_augmented25_norot.h5\")\n",
    "    y_train = np.reshape(y_train, newshape=(y_train.shape[0], 1))\n",
    "    y_test = np.reshape(y_test, newshape=(y_test.shape[0], 1))\n",
    "    num_classes = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def log_string(str):\n",
    "    logger.info(str)\n",
    "    print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETER ...\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(\"Model\")\n",
    "logger.setLevel(logging.INFO)\n",
    "experiment_dir = 'log/classification/' + log_dir\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler = logging.FileHandler('%s/eval.txt' % experiment_dir)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "log_string('PARAMETER ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting original data : 100%|██████████| 10/10 [00:00<00:00, 1152.76it/s]\n"
     ]
    }
   ],
   "source": [
    "MAX_SAMPLE = 10\n",
    "classifier = get_model(k=40, normal_channel=False)\n",
    "classifier.to(device)\n",
    "test_dataset = PoisonDataset(\n",
    "    data_set=list(zip(x_test[0:MAX_SAMPLE], y_test[0:MAX_SAMPLE])),\n",
    "    n_class=NUM_CLASSES,\n",
    "    target=TARGETED_CLASS,\n",
    "    name=\"test\",\n",
    "    is_sampling=False,\n",
    "    uniform=False,\n",
    "    data_augmentation=False,\n",
    "    is_testing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:00<00:06,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:01<00:05,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:02<00:04,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:02<00:04,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [00:03<00:03,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [00:04<00:02,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [00:04<00:02,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [00:05<00:01,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [00:06<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 1024)\n",
      "accuracy: 0.900000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    # batch_size=args.batch_size,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    ")\n",
    "print(len(test_dataset))\n",
    "\n",
    "checkpoint = torch.load(str(experiment_dir) + '/checkpoints/best_model.pth',\n",
    "                        map_location=lambda storage, loc: storage)\n",
    "classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "classifier.to(device)\n",
    "classifier = classifier.eval()\n",
    "sum_correct = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader):\n",
    "        points, label, mask = data\n",
    "        target = label[:, 0]\n",
    "        # pyplot_draw_point_cloud(points.numpy().reshape(-1, 3))\n",
    "        points = points.transpose(2, 1)\n",
    "        points, target = points.to(device), target.to(device)\n",
    "        predictions, feat_trans, hx, max_pool = classifier(points)\n",
    "        points = points.transpose(2, 1)\n",
    "        hx = hx.transpose(2, 1).cpu().numpy().reshape(-1, 1024)\n",
    "        print(hx.shape)\n",
    "        critical_mask = make_one_critical(hx=hx)\n",
    "        # visualize_point_cloud_critical_point(points.cpu().numpy().reshape(-1, 3), critical_mask)\n",
    "        pred_choice = predictions.max(1)[1]\n",
    "        # print(categories[pred_choice.cpu().numpy()[0]])\n",
    "        correct = pred_choice.eq(target.data).cpu().sum()\n",
    "\n",
    "        sum_correct += correct\n",
    "\n",
    "log_string('accuracy: %f' % (sum_correct / len(test_dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
